## Uninformed Search Strategies 
* Uninformed Search (blind search) : strategies have no additional information about states beyond that provided in the problem definition.

### Breadth-First Search 
* BFS : root node expanded then all sucessors of root expanded, then their successors and so on.
    * Shallowest unexpanded node chosen
    * Queue for frontier
![image](https://github.com/user-attachments/assets/6743134e-60cd-412f-95e8-ee9fb864bc37)

### Uniform-Cost Search (Dijkstras)
* Expand node n with lowest path cost g(n)
    * Priority Queue for frontier 
    * If all step costs are the same UCS is the same as BFS
![image](https://github.com/user-attachments/assets/44cae31e-87e0-493f-8298-c81f23d74e64)

### Depth-First Search
* Expands deepest node in frontier
    * Stack for frontier
* Backtracking Search : only one successor generated at a time rather than all successors
    * partially expanded nodes remember which successor to generate next
    * Memory saving

### Depth-Limited Search 
* DFS with predetermined depth limit l
    * solves DFS Problem in infinite state spaces 
![image](https://github.com/user-attachments/assets/9580e217-5e5f-478c-8118-8798e433e1eb)
* Diamater of state space : number of steps to reach any node from any other node, good choice for limiting depth if known

### Iterative Deepening Depth-First Search 
* DFS that finds the best depth limit by gradually increasing the limit from 0 up until a goal is found
![image](https://github.com/user-attachments/assets/20be2977-b1fd-40a5-a56b-c15fa14e1d6c)
* Preferred uninformed search method when the search space is large and the depth of the solution is not known
* Iterative Lengthening Search : UCS using increasing path-cost limits
    * substantial overhead compared to UCS

### Bidirectional Search
* Run two simultaneous searches : one forward from the initial state and the other backward from the goal hoping the two searches meet in the middle
    * Replaces goal test with check to see if the frontiers of the two searches intersect, if they do a solution has been found
* Requires method for computing predecessors
    * predecessors of a state x are all the states that have x as a successor 

### Comparing Uninformed Search Strategies 
![image](https://github.com/user-attachments/assets/abe92cc6-65e0-4d55-a5d4-7cbc21d168a0)

## Informed (Heuristic) Search Strategies 
* Informed Search (heuristic search) : strategies that know whether one non-goal state is more promising than another
* Best-First Search : node selected for expansion based on an evaluation function f(n) constructed as a cost estimate
* Heuristic Function h(n) = estimated cost of the cheapest path from the state at node n to a goal state
      * Most best first search algorithms include a heuristic function 

### Greedy Best-First Search 
* Expand node closest to the goal : f(n) = h(n)
* Straight-Line Distance Heuristic : staight line from start node to goal 

### A* Search: Minimizing the Total Estimated Solution Cost 
* f(n) = g(n) + h(n)
      * g(n) = cost to reach next node
      * h(n) = cost to get from node to goal
      * f(n) = estimated cost of the cheapest solution through n

Conditions for Optimality : Admissibility and Consistency
* Admissibile Heuristic : h(n) that never overestimates the cost to reach the goal
* Consistency (monotonicity) : every node n and every successor n' of n generated by any action a has estimated cost of reaching goal from n no greater than step cost of getting to n' plus the esimated cost of reaching the goal from n'
![image](https://github.com/user-attachments/assets/a7b5a5ec-7ac9-45ae-a2cb-03d3d09006d6)
      * required for A* to graph search
      * form of triangle inequality : stipulates that each side of a triangle cannot be longer than the sum of the other two sides 

Optimality of A*
* Tree search version of A* optimal if h(n) is admissible
* Graph search version of A* optimal if h(n) is consistent
* Pruning : eliminating possibilities from consideration without having to examine them
* A* optimally efficient for any given consistent heuristic  
* Absolute Error : delta = h* - h
      * h* is the actual cost of getting from the root to the goal
* Relative Error : ![image](https://github.com/user-attachments/assets/577554e0-fe64-4f10-9535-b2c99d7ac191)

### Memory-Bounded Heuristic Search 
* Iterative-Deepening A* (IDA*) : cutoff used is f-cost(g+h) rather than depth
      * cutoff value at each iteration is the smallest f-cost of any node that eceeded the cutoff on the previous iteration
* Recursive Best-First Search
      * Uses f_limit variable to keep track of the f-value of the best alternative path available from any ancestor of the current node. Backtracks if the current node exceeds this limit. 
![image](https://github.com/user-attachments/assets/7c81a91e-2586-4002-8b85-c2a2705e4e8f)
* Memory-Bounded A* (MA*) and simplified MA* (SMA*) use all available memory
      * IDA* and RBFS suffer from not utilizing enough memory 

### Learning to Search Better 
* Metalevel State Space : each state captures the internal computaitonal state of a program that is searching in an object-level state space
      * metalevel learning algorithm can learn from experience (hard steps) to avoid exploring unpromising subtrees
      * minimize total cost of problem solving, trading off computational expense and path cost 

## Heuristic Functions 
* Manhattan Distance : sum of vertical and horizontal distances to move from start to goal

### The Effect of Heuristic Accuracy on Performance 
* Effective Branching Factor b* ; branching factor that a uniform tree of depth d would have to have in order to contain N + 1 nodes 
* A Heuristic dominates another if for any node n h2(n) >= h1(n)
      * In this case h2 dominates h1

### Generating Admissible Heuristics from Relaxed Problems 
* Relaxed Problem : problem with fewer restrictions on the actions
      * Any optimal solution in the original problem is a solution in the relaxed problem because the relaxed problem adds edges to the state space
      * Relaxed problem can have better solutions if the additional edges create short cuts
![image](https://github.com/user-attachments/assets/0d175ac0-9bbf-4ee2-86dc-982016985a4b)

* Combine multiple heuristics by taking the max for better performance

### Generating Admissible Heuristics from Subproblems: Patern Databases 
* Pattern Databases store the exact solution costs for every possible subproblem instance
* Disjoint Pattern Databases : record cost of solving subproblem as just the number of moves involving specific elements
      * Easy to see if the sum of two costs is a lower bound on the cost of solving the entire problem

### Learning HEuristics from Experience 
* Experience means solving lots of problems
      * A learning algorithm can use these examples to construct a function h(n) that with some luck can predict solution costs for other states that arise during search
* Works best when supplied features of a state relevant to predicitng the state's value
